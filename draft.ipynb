{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/spaces/santapo/anaconda3/envs/hackathon/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "sentence = 'Chúng_tôi là những nghiên_cứu_viên .'  \n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(sentence)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples\n",
    "\n",
    "## With TensorFlow 2.0+:\n",
    "# from transformers import TFAutoModel\n",
    "# phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs = torch.concat([input_ids, input_ids], dim=0)\n",
    "batch_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = phobert(batch_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3267])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = \"false\"\n",
    "input_1 = torch.tensor([tokenizer.encode(word_1)])\n",
    "\n",
    "word_2 = \"\"\n",
    "input_2 = torch.tensor([tokenizer.encode(word_2)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    features_1 = phobert(input_1)\n",
    "    features_2 = phobert(input_2)\n",
    "\n",
    "F.cosine_similarity(features_1[1], features_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000044vscode-remote?line=0'>1</a>\u001b[0m [\u001b[39m\"\u001b[39;49m\u001b[39mASLDKFJAL\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mAJOSDHFAOSDKJ\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mlower()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "[\"ASLDKFJAL\", \"AJOSDHFAOSDKJ\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chúng tôi là những nghiên cứu viên 9873  7 '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use', 'proficiently', 'kjahgkdsajadsbfksd', 'Pascal', 'C', 'C', 'Java', 'C', 'Python', 'programming', 'languages', 'PRODUCTS', 'Web', 'Programming', 'HTML', 'CSS', 'JS', 'JQ', 'AI', 'programming', 'mainly', 'computer', 'vision', 'C', 'Python', 'Deployment', 'AI', 'code', 'Deepstream', 'triton', 'rabitMQ', 'elasticsearch', 'NCNN']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "sentence = \"- Use proficiently kjahgkdsajadsbfksd  Pascal, C, C++, Java, C#, Python programming languages\\nPRODUCTS\\n- Web Programming (HTML,CSS,JS,JQ)\\n- AI programming (mainly computer vision)\\n(C++,Python)\\n- Deployment AI code (Deepstream , triton,\\nrabitMQ, elasticsearch, NCNN,...)\"\n",
    "sentence = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ', sentence)\n",
    "# sentence = \"Trung thực, năng động, tự tin, sáng tạo, có tinh thần trách nhiệm cao trong  công việc.\\nGiao tiếp tốt, có sẵn mối quan hệ với các hệ thống siêu thị.\\nCó khả năng đàm phán thương thuyết với khách hàng.\\nThành thạo vi tính văn phòng.\\nCó khả năng lãnh đạo nhóm, phát triển cũng như huấn luyện cho nhân viên  cấp dưới.\\nCó kinh nghiệm lâu năm trong việc quản lý nhân sự\"\n",
    "# sentence = \"Đọc sách. Thư giãn bằng nhạc không lời. Tập thể dục buổi sáng và buổi chiều\\nDu lịch và khám phá các miền đất mới. Được học hỏi và train các kỹ năng mới\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "stop_words = set(stopwords.words('vietnamese'))\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [w for w in tokens if w not in stop_words]\n",
    "sentence = ' '.join(words)\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02247191011235955"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "word_1 = \"\\u2022 An active team member with ability to handle problems individually\\n\\u2022 Strong communication skills\\n\\u2022 Fluent in the English language\\n\\u2022 Experience with using Microsoft office: Word, Excel, PP  \\u2022 Hardworking and able to work long hours when necessary\"\n",
    "word_2 = \"Microsoft Word Excel\"\n",
    "SequenceMatcher(None, word_2, word_1).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6684])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_2[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m F\u001b[39m.\u001b[39;49mcosine_similarity(features_1[\u001b[39m0\u001b[39;49m], features_2[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "F.cosine_similarity(features_1[0], features_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 746, 8, 21, 46349, 5, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 28138, 55759,  2139,     2]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(word_1, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 572/572 [00:00<00:00, 177kB/s]\n",
      "Downloading: 100%|██████████| 1.12G/1.12G [01:24<00:00, 14.2MB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000026vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000026vscode-remote?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mNlpHUST/t5-small-vi-summarization\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000026vscode-remote?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mNlpHUST/t5-small-vi-summarization\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000026vscode-remote?line=6'>7</a>\u001b[0m src \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCreative and Production \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m- Head Ho Chi Minh City, Vietnam September 2011 \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m- March 2015 (3 years 7 months) Ho Chi Minh City, Vietnam Mead Johnson Nutrition Management Trainee October 2010 \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m- August 2011 (11 months) Ho Chi Minh city\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btechainer_1080ti_1/mnt/hdd/spaces/santapo/hackathon_master/resume-ranking/draft.ipynb#ch0000026vscode-remote?line=7'>8</a>\u001b[0m tokenized_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(src, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hackathon_master/transformers/src/transformers/utils/import_utils.py:794\u001b[0m, in \u001b[0;36mDummyObject.__getattr__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/hdd/spaces/santapo/hackathon_master/transformers/src/transformers/utils/import_utils.py?line=791'>792</a>\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///mnt/hdd/spaces/santapo/hackathon_master/transformers/src/transformers/utils/import_utils.py?line=792'>793</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39mcls\u001b[39m, key)\n\u001b[0;32m--> <a href='file:///mnt/hdd/spaces/santapo/hackathon_master/transformers/src/transformers/utils/import_utils.py?line=793'>794</a>\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/hackathon_master/transformers/src/transformers/utils/import_utils.py:782\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/hdd/spaces/santapo/hackathon_master/transformers/src/transformers/utils/import_utils.py?line=779'>780</a>\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m    <a href='file:///mnt/hdd/spaces/santapo/hackathon_master/transformers/src/transformers/utils/import_utils.py?line=780'>781</a>\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m--> <a href='file:///mnt/hdd/spaces/santapo/hackathon_master/transformers/src/transformers/utils/import_utils.py?line=781'>782</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
    "\n",
    "\n",
    "src = \"Creative and Production \\n- Head Ho Chi Minh City, Vietnam September 2011 \\n- March 2015 (3 years 7 months) Ho Chi Minh City, Vietnam Mead Johnson Nutrition Management Trainee October 2010 \\n- August 2011 (11 months) Ho Chi Minh city\"\n",
    "tokenized_text = tokenizer.encode(src, return_tensors=\"pt\")\n",
    "model.eval()\n",
    "summary_ids = model.generate(\n",
    "                    tokenized_text,\n",
    "                    max_length=256, \n",
    "                    num_beams=5,\n",
    "                    repetition_penalty=2.5, \n",
    "                    length_penalty=1.0, \n",
    "                    early_stopping=True\n",
    "                )\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "141a2bbbc9c5f8fa7756d79da00cd7fc280550beb6f4beea5dccaf5f5664c8c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hackathon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
